{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmvF4blMS-Iw"
      },
      "source": [
        "# Лабораторная работа №6\n",
        "\n",
        "**Сверточные нейронные сети**\n",
        "\n",
        "---\n",
        "\n",
        "**Впишите в эту ячейку ваши ФИО, группу**.\n",
        "\n",
        "ФИО: Сильченко Алексей Евгеньевич \n",
        "\n",
        "Группа: 201-361\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JW2lM4yYXio"
      },
      "source": [
        "## Загрузка данных\n",
        "\n",
        "В данной работе будет использоваться учебный датасет с изображениями персонажей из Симпсонов. Код для скачивания и распаковки приведен ниже, его требуется только выполнить и вo вкладке Files должна появиться папка `data`, а в ней папки `train` и `test`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhBI_Y2iVHqO",
        "outputId": "fe37f0d1-8509-4ab1-bde3-0124598070c3"
      },
      "outputs": [],
      "source": [
        "#Загрузка для windows\n",
        "# Invoke-WebRequest -Uri http://labcolor.space/rgb-test.zip -OutFile rgb-test.zip\n",
        "# Expand-Archive -Path rgb-test.zip -DestinationPath data -Force\n",
        "\n",
        "# Invoke-WebRequest -Uri http://labcolor.space/rgb-train.zip -OutFile rgb-train.zip\n",
        "# Expand-Archive -Path rgb-train.zip -DestinationPath data -Force"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWExBZ0FZj1E"
      },
      "source": [
        "## Создание объекта Dataset\n",
        "\n",
        "Так как изображения в датасете организованы по папкам, где имя папки является ярлыком для данных, то мы можем воспользоваться базовым классом `ImageFolder`.\n",
        "\n",
        "Одним из параметров является `transform`, для которого необходимо скомпоновать преобразования для наших изображений. В pytorch для преобразований сейчас есть два набора функций V1 и V2 и рекомендуется использовать V2, хоть напротив многих функций указано состояние beta.\n",
        "\n",
        "Для компоновки функции из модуля v2 используйте `Compose`. Вам понадобится обязательно:\n",
        "* ToImage() - преобразование в `Image` (подкласс torch.Tensor)\n",
        "* RandomVerticalFlip() - случайное отзеркаливание\n",
        "* ToDtype(torch.float32, scale=True) - преобразование из int во float\n",
        "* Normalize() - нормализация изображений по полученным средним и стандартным отклонениям.\n",
        "\n",
        "По желанию:\n",
        "* RandomRotation() - поворот на случайный угол в указанном диапазоне\n",
        "* Можете попробовать и другие варианты преобразований. [Документация API V2](https://pytorch.org/vision/stable/transforms.html#v2-api-reference-recommended)\n",
        "\n",
        "В качестве примера будет показана работа с созданием Dataset для получения статистик изображения. Вам же необходимо будет создать `transforms` для обучения и проверки. При обучении вы используете весь набор обязательных преобразований, при обучении вам требуется только преобразовать изображение к тензору с плавающей точкой и провести нормализацию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2s7NaAvVHH6V"
      },
      "source": [
        "### Получение статистик для нормализации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HHN2GrMjalJy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import v2\n",
        "\n",
        "transforms_stats = v2.Compose([\n",
        "    v2.ToImage(),\n",
        "    v2.ToDtype(torch.float32, scale=True),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "28mPRQLMHCnd"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "stats_dataset = ImageFolder(root=\"./data/train\", transform=transforms_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F73DC1a6HByF"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Means R, G, B: (0.465204, 0.40914717, 0.35447764)\n",
            "Std R, G, B: (0.2492806, 0.22941421, 0.2450912)\n"
          ]
        }
      ],
      "source": [
        "# Собираем все изображения из датасета в список, где каждый элемент - это изображение.\n",
        "imgs = [item[0] for item in stats_dataset]\n",
        "\n",
        "# Преобразуем список изображений в тензор PyTorch для дальнейших вычислений,\n",
        "# объединяя их в один тензор с добавлением нового измерения в начале,\n",
        "# что соответствует размеру пакета (batch size).\n",
        "imgs = torch.stack(imgs, dim=0).numpy()\n",
        "\n",
        "# Вычисляем среднее значение пикселей канала красного, зеленого и синего по всем изображениям.\n",
        "mean_r = imgs[:,0,:,:].mean()\n",
        "mean_g = imgs[:,1,:,:].mean()\n",
        "mean_b = imgs[:,2,:,:].mean()\n",
        "\n",
        "# Выводим средние значения для каждого из цветовых каналов.\n",
        "print(f\"Means R, G, B: {mean_r,mean_g,mean_b}\")\n",
        "\n",
        "# Вычисляем стандартное отклонение пикселей канала красного, зеленого и синего по всем изображениям.\n",
        "std_r = imgs[:,0,:,:].std()\n",
        "std_g = imgs[:,1,:,:].std()\n",
        "std_b = imgs[:,2,:,:].std()\n",
        "\n",
        "# Выводим значения стандартных отклонений для каждого из цветовых каналов.\n",
        "print(f\"Std R, G, B: {std_r,std_g,std_b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Z7YQmBiHehJ"
      },
      "source": [
        "**Почему значения средних и стандартных отклонений мы получаем только для обучающей выборки?**\n",
        "\n",
        "Ваш ответ: Нормализация данных(Средние и стандартные отклонения используются для нормализации данных обучающей выборки), предотвращение утечки данных, соответствие реальным условиям, универсальность модели.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Используйте выведенные выше значения средних и стандартных отклонений в качестве аргументов функции `Normalize`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "E9zeyUxyGcrZ"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import v2 as transforms\n",
        "\n",
        "means = (0.465204, 0.40914717, 0.35447764) # Средние значения для каналов RGB\n",
        "stds = (0.2492806, 0.22941421, 0.2450912) # Стандартные отклонения для каналов RGB\n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "    #transforms.Resize(256), # Изменение размера всех изображений до 256 пикселей по меньшей стороне\n",
        "    #transforms.RandomCrop(224), # Случайное обрезание изображений до размера 224x224 пикселей\n",
        "    transforms.RandomVerticalFlip(), # Случайное вертикальное отражение изображений\n",
        "    transforms.ToImage(), # Преобразование в объекты изображения PIL\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean=means, std=stds),\n",
        "])\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "    #transforms.Resize(224), # Изменение размера всех изображений до 224 пикселей по меньшей стороне\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean=means, std=stds),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVSanpdBlJpP"
      },
      "source": [
        "Теперь, когда есть необходимые `transforms` можно создать ImageFolder, указав в `root` путь до выборки и `transforms` в `transform`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "jnrdWtQCZHY1"
      },
      "outputs": [],
      "source": [
        "\n",
        "train_data_path = './data/train'\n",
        "val_data_path = './data/test'\n",
        "\n",
        "train_dataset = ImageFolder(root=train_data_path, transform=transforms_train)\n",
        "\n",
        "val_dataset = ImageFolder(root=val_data_path, transform=transforms_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KIMD10uQd14C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Количество изображений в обучающей выборке: 8000\n",
            "Количество изображений в тестовой выборке: 2000\n",
            "Количество классов: 10\n",
            "Количество каналов в изображении: 3\n",
            "Высота и ширина изображения: 224x224\n"
          ]
        }
      ],
      "source": [
        "mean = [0.465204, 0.40914717, 0.35447764]\n",
        "std = [0.2492806, 0.22941421, 0.2450912]\n",
        "\n",
        "# Определение преобразований\n",
        "transforms = v2.Compose([\n",
        "    v2.ToTensor(),\n",
        "    v2.Normalize(mean=mean, std=std),\n",
        "])\n",
        "\n",
        "# Создание датасетов\n",
        "train_dataset = ImageFolder(root=\"./data/train\", transform=transforms)\n",
        "test_dataset = ImageFolder(root=\"./data/test\", transform=transforms)\n",
        "\n",
        "# Получение информации\n",
        "num_train_images = len(train_dataset)\n",
        "num_test_images = len(test_dataset)\n",
        "num_classes = len(train_dataset.classes)\n",
        "# Получение случайного изображения из обучающего набора для определения размеров и количества каналов\n",
        "image, _ = train_dataset[0]\n",
        "channels, height, width = image.shape\n",
        "print(f'Количество изображений в обучающей выборке: {num_train_images}')\n",
        "print(f'Количество изображений в тестовой выборке: {num_test_images}')\n",
        "print(f'Количество классов: {num_classes}')\n",
        "print(f'Количество каналов в изображении: {channels}')\n",
        "print(f'Высота и ширина изображения: {height}x{width}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhGqUh4youqC"
      },
      "source": [
        "## Создание DataLoader\n",
        "\n",
        "Далее необходимо подготовить три загрузчика данных:\n",
        "\n",
        "1. Обучающий\n",
        "2. Проверочный\n",
        "3. Тестовый\n",
        "\n",
        "Тестовый загрузчик делается из тестового Dataset, а обучающий и проверочный необходимо создать, используя [SubsetRandomSampler](https://pytorch.org/docs/stable/data.html#torch.utils.data.SubsetRandomSampler), для его работы требуется массив индексов, по которым в дальнейшем загрузчик будет в случайном порядке брать изображения и лейблы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7_S19XDhmdhP"
      },
      "outputs": [],
      "source": [
        "# Импорт необходимых библиотек\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "# Определение размера валидационного набора как 20% от общего количества образцов\n",
        "val_size = 0.2\n",
        "\n",
        "# Определение общего числа образцов в обучающем наборе данных\n",
        "train_samples = len(train_dataset)\n",
        "# Создание списка индексов для всех образцов в обучающем наборе\n",
        "train_indices = list(range(train_samples))\n",
        "\n",
        "# Вычисление индекса, который будет использоваться для разделения на обучающий и валидационный наборы\n",
        "split_value = int(np.floor(val_size * train_samples))\n",
        "# Перемешивание индексов для обеспечения случайности разделения\n",
        "np.random.shuffle(train_indices)\n",
        "\n",
        "# Разделение индексов на обучающие и валидационные, используя вычисленный индекс разделения\n",
        "# Обучающие индексы получаются путем выбора элементов после индекса разделения\n",
        "train_idx, val_idx = train_indices[split_value:], train_indices[:split_value]\n",
        "\n",
        "# Создание объектов Sampler для обучающего и валидационного наборов\n",
        "# Эти объекты будут случайным образом выбирать элементы из соответствующих индексов\n",
        "train_sampler = SubsetRandomSampler(train_idx)\n",
        "val_sampler = SubsetRandomSampler(val_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqeo5E_dLAD4"
      },
      "source": [
        "**Опишите своими словами, что делает каждая строчка кода в предыдущей ячейке.**\n",
        "\n",
        "Ваш ответ: Расписано в коментариях"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Bq9fnX6ppC-V"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Определяем размер батча (количество образцов данных, обрабатываемых за одну итерацию)\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Создаём DataLoader для обучающего набора данных.\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "# Создаём DataLoader для валидационного набора данных.\n",
        "val_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=val_sampler)\n",
        "# Создаём DataLoader для валидационного набора данных.\n",
        "test_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdbG4_QkW1Xp"
      },
      "source": [
        "**Какую задачу решает Dataloader?**\n",
        "\n",
        "Ваш ответ:\n",
        "1. Автоматизация батчинга: DataLoader автоматически группирует данные в батчи заданного размера, что важно для эффективного обучения моделей глубокого обучения, поскольку обработка данных пакетами помогает оптимизировать вычислительные ресурсы.\n",
        "\n",
        "2. Перемешивание данных: В случае обучающего набора данных DataLoader может перемешивать данные перед формированием батчей. Это предотвращает модель от запоминания порядка следования данных и способствует лучшему обобщению.\n",
        "\n",
        "3. Параллельная загрузка и предобработка: DataLoader поддерживает многопоточную загрузку данных, что снижает время ожидания ввода данных и ускоряет обучение. Предобработка данных (например, нормализация, аугментация) также может быть интегрирована и выполнена параллельно.\n",
        "\n",
        "**Почему использование трех выборок (обучающей, валидационной, тестовой) считается хорошей практикой?**\n",
        "\n",
        "Ваш ответ:\n",
        "1. Обучающая выборка: Используется непосредственно для обучения модели. Это основной набор данных, на котором модель \"учится\" на примерах, адаптируя свои веса для минимизации ошибки.\n",
        "\n",
        "2. Валидационная выборка: Используется для оценки модели в процессе обучения, но не для обучения. Основная цель валидационной выборки — помочь в настройке гиперпараметров модели и предотвратить переобучение. Модель не \"видит\" данные из валидационной выборки в процессе обучения, что позволяет проверить, насколько хорошо модель обобщает на новые данные.\n",
        "\n",
        "3. Тестовая выборка: Используется после завершения процесса обучения для окончательной оценки модели. Тестовая выборка позволяет проверить, насколько эффективно модель работает на данных, которые никогда не использовались в процессе обучения или настройки. Это конечная проверка обобщающей способности модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1diJ0WPML5WJ"
      },
      "source": [
        "## Создание модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISx1-JU_NIFA"
      },
      "source": [
        "За основу можно взять модель LeNet-5, но скорее всего вам придется ее адаптировать под свою задачу, так как в большинстве случаев она написана под черно-белые изображения размером 32 на 32 пикселя.\n",
        "\n",
        "Сверточные нейронные сети состоят из двух частей:\n",
        "1. Слои свертки(функции свертки, активации, субдискретизации)\n",
        "2. Полносвязные слои (MLP)\n",
        "\n",
        "Слои можно объединить с помощью `nn.Sequential()`. А класс вашей модели должен наследоваться от `nn.Module`.\n",
        "\n",
        "`def forward()` определяет прямой ход и должна возвращать итоговый результат работы модели - в данном случае логиты."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "KW3nfcAZL8XV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # Определение первого свёрточного слоя\n",
        "        # 3 входных канала (для RGB изображений), 16 выходных каналов, ядро размером 5,\n",
        "        # шаг свёртки 1 и паддинг 2 для сохранения размерности изображения.\n",
        "        # После свёртки применяется функция активации ReLU и операция максимального пулинга с ядром 2 и шагом 2.\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        # Определение второго свёрточного слоя, аналогичного первому,\n",
        "        # но с 32 выходными каналами.\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        \n",
        "        # Определение полносвязных слоёв:\n",
        "        # fc1 преобразует данные из двумерного формата в вектор, fc2 и fc3 далее уменьшают размерность,\n",
        "        # ведущую к выходному слою, который соответствует количеству классов.\n",
        "        # Важно отметить, что размер входа в fc1 должен соответствовать размеру данных после свёрточных и пулинг слоёв.\n",
        "        self.fc1 = nn.Linear(32 * 56 * 56, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    # Определение прямого прохода через сеть\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x) # Проход через первый свёрточный слой\n",
        "        out = self.layer2(out) # Проход через второй свёрточный слой\n",
        "        \n",
        "        out = out.view(out.size(0), -1) # Преобразование данных для полносвязного слоя\n",
        "        \n",
        "        out = F.relu(self.fc1(out)) # Проход через первый полносвязный слой с активацией ReLU\n",
        "        out = F.relu(self.fc2(out)) # Проход через второй полносвязный слой с активацией ReLU\n",
        "        out = self.fc3(out) # Выход через последний полносвязный слой без активации\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W96DOSUzV43g"
      },
      "source": [
        "**Опишите суть операции свертки.**\n",
        "\n",
        "Ваш ответ: \n",
        "- Операция свёртки в контексте обработки изображений и глубокого обучения представляет собой математическую процедуру, при которой изображение \"просматривается\" через так называемое свёрточное ядро (или фильтр) для получения новой матрицы, называемой картой признаков. Это ядро имеет меньшие размеры по сравнению с исходным изображением и \"скользит\" по всему изображению, вычисляя в каждом положении сумму произведений его значений и соответствующих значений изображения под ним.\n",
        "\n",
        "- Процесс свёртки помогает выявить определённые характеристики изображения, такие как границы, углы или текстуры, в зависимости от типа применяемого фильтра. При обучении нейронной сети параметры этих фильтров оптимизируются таким образом, чтобы лучше всего выделять полезные для задачи признаки.\n",
        "\n",
        "**Опишите суть операции субдискретизации.**\n",
        "\n",
        "Ваш ответ: Операция субдискретизации, также известная как пулинг, является процессом уменьшения размерности полученной карты признаков, сохраняя при этом важные информационные сигналы.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH1LAGLEN6T7"
      },
      "source": [
        "## Создание объекта модели, функции потерь и оптимизатора\n",
        "\n",
        "В качестве функции потерь будет использована перекрестная энтропия, в задании MLP вы фактически ее реализовали, но через набор отдельных функций.\n",
        "\n",
        "В качестве оптимизатора можете взять стохастический градиентный спуск или Adam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "OJarp7KJN6pr"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Создание экземпляра модели\n",
        "model = ConvNet(num_classes=10)\n",
        "\n",
        "# Функция потерь\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Оптимизатор\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3d1eIAOPEiJ"
      },
      "source": [
        "## Цикл обучения\n",
        "\n",
        "Попробуйте разные гиперпараметры для вашей модели. Попробуйте улучшить результат от первой попытки.\n",
        "\n",
        "Вы можете по своему желанию добавить графики потерь и точности от эпохи."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "dLSoxR7ePE1_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.9978\n",
            "Accuracy of the model on the validation images: 44.31 %\n",
            "Epoch [2/10], Loss: 1.5519\n",
            "Accuracy of the model on the validation images: 50.94 %\n",
            "Epoch [3/10], Loss: 1.3807\n",
            "Accuracy of the model on the validation images: 56.25 %\n",
            "Epoch [4/10], Loss: 1.2333\n",
            "Accuracy of the model on the validation images: 57.06 %\n",
            "Epoch [5/10], Loss: 1.1660\n",
            "Accuracy of the model on the validation images: 60.50 %\n",
            "Epoch [6/10], Loss: 1.0635\n",
            "Accuracy of the model on the validation images: 58.00 %\n",
            "Epoch [7/10], Loss: 1.0012\n",
            "Accuracy of the model on the validation images: 62.19 %\n",
            "Epoch [8/10], Loss: 0.9349\n",
            "Accuracy of the model on the validation images: 63.94 %\n",
            "Epoch [9/10], Loss: 0.8875\n",
            "Accuracy of the model on the validation images: 63.62 %\n",
            "Epoch [10/10], Loss: 0.8531\n",
            "Accuracy of the model on the validation images: 65.94 %\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10 #кол-во эпох\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Переключаем модель в режим обучения\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Прямой ход\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        # Обнуление градиентов\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Обратный ход\n",
        "        loss.backward()\n",
        "\n",
        "        # Шаг оптимизатора\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Валидация\n",
        "    model.eval()  # Переключаем модель в режим валидации\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in val_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the validation images: {accuracy:.2f} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcqG7WGtPnCz"
      },
      "source": [
        "## Итоговая оценка\n",
        "\n",
        "Часть кода, которую вы реализовали для оценки модели на валидационной выборке, можно использовать для финальной проверки, указав нужный loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iXZ7EfU7Pvtz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Точность (accuracy) на тестовом наборе данных: 66.45%\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()  # Переключение модели в режим оценки\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:  # Использование DataLoader тестового набора\n",
        "        outputs = model(images)  # Расчет вывода модели для данного батча\n",
        "        _, predicted = torch.max(outputs.data, 1)  # Получение индексов максимальных значений для предсказаний\n",
        "        total += labels.size(0)  # Подсчет общего количества меток\n",
        "        correct += (predicted == labels).sum().item()  # Подсчет количества правильных предсказаний\n",
        "\n",
        "    print(f'Точность (accuracy) на тестовом наборе данных: {100 * correct / total}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5oB2n8VyNnT"
      },
      "source": [
        "**Точность работы модели на тестовой выборке**\n",
        "\n",
        "Ваш ответ: 66.45%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
